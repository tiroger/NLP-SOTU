{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing dependencies\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import requests \n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting transcript for 1793\n",
      "Getting transcript for 1794\n",
      "Getting transcript for 1795\n",
      "Getting transcript for 1796\n",
      "Successfully scrapped 4 records\n"
     ]
    }
   ],
   "source": [
    "sotu_raw_transcripts = []\n",
    "\n",
    "dates = list(range(1793, 1797)) # Dates 1793-2020\n",
    "dates_str = [str(d) for d in dates]\n",
    "dates_str\n",
    "\n",
    "for date in dates_str:\n",
    "    print(f'Getting transcript for {date}')\n",
    "    try:\n",
    "        # Opening URL\n",
    "        base_url = 'https://www.presidency.ucsb.edu/documents/presidential-documents-archive-guidebook/annual-messages-congress-the-state-the-union'\n",
    "        driver = webdriver.Chrome('./chromedriver')\n",
    "        driver.get(base_url)\n",
    "        driver.implicitly_wait(10) # seconds\n",
    "        element = driver.find_element_by_link_text(date)\n",
    "        webdriver.ActionChains(driver).move_to_element(element).click(element).perform()\n",
    "        current_url = driver.current_url # url containing transcript of SOTU addresses\n",
    "        #print(current_url)\n",
    "        r = requests.get(current_url)\n",
    "        # print(r.content)\n",
    "        soup = BeautifulSoup(r.content, 'html5lib') \n",
    "        #print(soup.prettify())\n",
    "        # Using BeautifulSoup to extract transcript\n",
    "        text = soup.find('div', class_='field-docs-content').text\n",
    "        text = soup.find('div', class_='node-documents').text\n",
    "        sotu_raw_transcripts.append(text)\n",
    "        driver.close()\n",
    "    except:\n",
    "        print(f'Could not get record for {date}')\n",
    "#print(sotu_transcripts)\n",
    "num_records = len(sotu_transcripts)\n",
    "print(f'Successfully scrapped {num_records} records')\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pickling raw transcripts\n",
    "with open('./pickled_files/sotu_raw_transcripts.pkl', 'wb') as f:\n",
    "    pickle.dump(sotu_raw_transcripts, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PythonData",
   "language": "python",
   "name": "pythondata"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
